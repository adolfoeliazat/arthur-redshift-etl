{
    # Target (Redshift) cluster
    "data_warehouse": {
        # The environment variable must contain a full connection string for an admin user to create a database.
        "admin_access": "DATA_WAREHOUSE_ADMIN",
        # The environment variable must contain a full connection string for an ETL user.
        "etl_access": "DATA_WAREHOUSE_ETL",
        # The environment variable must contain a full connection string for a reference database (for comparing state)
        "reference_warehouse_access": "REFERENCE_WAREHOUSE_ETL",
        # All schemas, tables, etc. will be assigned to this user.  The owner's group will be the ETL group.
        "owner": {
            "name": "dw",
            "group": "etl_rw"

        },
        # Schemas that are filled out based on CTAS or views and have no direct upstream source
        "schemas": []
    },
    # Logging of the ETL into events tables
    "etl_events": {
        # Send ETL events to DynamoDB table
        "dynamodb": {
            "table_prefix": "redshift-etl-events",
            "capacity": 3,
            "region": "us-east-1"
        # },
        # # Send ETL events to an RDS instance / PostgreSQL host
        # "postgresql": {
        #     "table_prefix": "redshift_etl_events",
        #     "write_access": "REDSHIFT_ETL_EVENTS_URI"
        }
    },
    # Type information from source (PostgreSQL) to target (Redshift)
    "type_maps": {
        # Types that may be used "as-is", see also
        # http://docs.aws.amazon.com/redshift/latest/dg/c_Supported_data_types.html
        # The keys are regular expression (with escaped backslashes!) and the
        # values are the serialization formats in Avro files.
        "as_is_att_type": {
            "integer": "int",
            "bigint": "long",
            "double precision": "double",
            "boolean": "boolean",
            "character\\(\\d+\\)": "string",
            "character varying\\(\\d+\\)": "string",
            "date": "string",
            "timestamp without time zone": "string",
            "numeric\\(\\d+,\\d+\\)": "string"
        },
        # Map of known PostgreSQL attribute types to usable types in Redshift.  Missing types will cause an exception
        # The first element in the list is the new type, the second element is the necessary cast expression,
        # the third element is the serialization format in Avro files.
        # Note that in any expression, %s is replaced by the column name within quotes.
        "cast_needed_att_type": {
            "int4range": ["varchar(65535)", "%s::varchar(65535)", "string"],
            "integer\\[\\]": ["varchar(65535)", "%s::varchar(65535)", "string"],
            "bigint\\[\\]": ["varchar(65535)", "%s::varchar(65535)", "string"],
            # NOTE varchar counts characters but Redshift is byte limitations
            "character varying": ["varchar(65535)", "%s::varchar(65535)", "string"],
            "text": ["varchar(65535)", "%s::varchar(65535)", "string"],
            "time without time zone": ["varchar(256)", "%s::varchar(256)", "string"],
            # CAVEAT EMPTOR This only works if your database is running in UTC.
            "timestamp with time zone": ["timestamp without time zone", "%s::varchar(256)", "string"],
            "json": ["varchar(65535)", "%s::varchar(65535)", "string"],
            # N.B. Newer versions of PostgreSQL have a more useful hstore_to_json function but this works for 9.2
            "hstore": ["varchar(65535)", "public.hstore_to_matrix(%s)::varchar(65535)", "string"],
            "uuid": ["varchar(36)", "%s::varchar(36)", "string"],
            # The numeric datatype without precision and scale should not be used upstream!
            "numeric": ["decimal(12,4)", "%s::decimal(12,4)", "string"]
        }
    }
}
